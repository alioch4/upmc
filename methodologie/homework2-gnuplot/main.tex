\documentclass{sig-alternate-10pt}

\usepackage{graphicx}
\usepackage{listings}
\usepackage{hyperref}

\begin{document}

\title{Homework - Making the point}

\numberofauthors{1} %  in this sample file, there are a *total*

\author{
  \alignauthor
    Rémy Léone\\
    \affaddr{UPMC~-- M2 Recherche~-- METHO}\\
    \email{remy.leone@gmail.com}
}

\maketitle

\section{Analysis of scientist conversation distribution}

\subsection{Experimental method}

We used several script to extract information from the data provided before this study.
First we read the main data using a python script, then we process it by creating some 
new variables (total time of conversation, mean time of conversation). Then we normalize
the data using R programming language. Finally we use gnuplot to create graphical representation
of our data.

\subsection{Results analysis}

We obtain a very heterogeneous set of results. Several behavior could be observed in our result :
\begin{description}
\item Average number of connections with other scientists
\item Lower than average connections
\item Above average connections
\end{description}

Most of the scientists attending to this conference seems to have a very average behavior
(having around 125 conversations). Nevertheless, some of them are having more conversations
and during a longer period. It's clear by looking at the deviation from the mean value of the 
number of conversation and the mean time of conversation. We can also noticed some very low
results from some scientists. There is some outliers, two of them are a complete lack of conversations
giving two missing point, and one of them is a high values chatting person.

\begin{figure}[h]
\begin{center}
\includegraphics{img/part1}
\end{center}
\caption{Normalized results of conversation}
\end{figure}

We can also notice that some scientists are not talking at all, because we don't know 
a lot about how this data was created we can suppose it could be some errors.

\subsection{Conclusion and open questions}

Other informations could be required to analyse more deeply this conference and answer more questions. 
For instance, we don't 
have any information about the scientists. It could be very interesting to compare this conversations
with the administrative position of each of them (graduate student, senior, laboratory leader).

A study of the chat distribution could be interesting. Suppositions can be made about the correlation
between the administrative responsability and the chat-appeal a person could have in a science conference.

Some results could be obtained by looking at the time distribution, maybe there is some period where
no communications appends and some others that are very dense. Linking this results

\lstinputlisting[language=Gnuplot]{scripts/script.gnuplot}

\section{Availability in large scale distributed networks}

SETI@home project was launched in May 1999 and have used to study the behavior of 
large scale distributed network (\cite{ja_ko_mascots09}). The main problem of SETI@home
(and all benevolant projects alike) is the availability of node that could evolve very quickly.
As a result the average availability could suffer from a massive disconnection of nodes.

Boinc platform (230 000) hosts
April 1 2007 -> January 1 2009
CPU FREE or not
57 800 years of CPU time
102 416 434 continuous intervals

We could compare the average result of a network with similarity with SETI@home and a classical
network (A supercomputer or a cluster of university's computers).

\subsection{Experimental method}

The massive amount of data that were used by previous work on distributed computing (\cite{ja_ko_mascots09})
was used also in this paper. A GNU/Linux computer with an Intel i5 CPU and 4 Gb of ram was used to conduct this
experiment. Python, R and other free software scripting tools have benn used to conduct this experiments.

The complete dataset is quite huge (around 17 Gb), a computer with much more memory would be appreciated to
compute more advance results. A smaller set of data were used to do the computation. With only 226207 lines 
of information the subset of the component is a more easily to handle set of data

\subsection{Experimental results}

Only \texttt{component.tab} was used to analyse the normalized density function of the connexion time.

An interesting perspective could be to recreate the clusetering that occurs from 2010 to 2011 to see if
the results are different from what happened between 2007 and 2009. A lot of computationnal power could be 
required to apply clustering algorithm on this massive set of data.




\begin{figure}[h]
\begin{center}
\includegraphics[scale=0.5]{img/part2}
\end{center}
\caption{Normalized results of uptime}
\label{part2}
\end{figure}


\subsection{Conclusion}

As showed in \ref{part2}


\subsection{Code sample}

All source code is available on 
\href{https://github.com/sieben/upmc/tree/master/methodologie/homework2-gnuplot}{github}.

\subsubsection{Extraction and processing script}

\lstinputlisting[language=Python]{scripts/seti.py}

\subsubsection{Statistical processing}

\lstinputlisting[language=R]{scripts/seti.R}

\bibliographystyle{abbrv}
\bibliography{main}


\end{document}
